{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Keras Models\n",
    "\n",
    "Welcome!  Today we're going to build on the previous lessons by making improvements to the way we make and build tensorflow models with Keras.\n",
    "\n",
    "To recap:\n",
    " - We've discussed the basics of an NLP model with two different types of network layers:\n",
    "   - `word embedding`:  the docking stations for each word in our training corpus, with their assigned set of weights\n",
    "   - `dense layers`: the 'standard' neural network layer, which multiplies the incoming data by a block of weights in a linear fashion\n",
    " - We've discussed the basics of how to prep word data for said model:\n",
    "   - cleaning up unnecessary characters due to formatting issues\n",
    "   - tokenizing our data so that each word is mapped to its own index position based on its frequency in the training corpus\n",
    "   - padding our word sequences so that they all have equal length\n",
    "   \n",
    " - We've also discussed some other nuts and bolts of neural networks:\n",
    "   - `activation functions`:  data transformations that happen inbetween layers to introduce non-linearity to assist in pattern recognition;  sometimes also to coerce data into a particular format;\n",
    "     - in the broadest terms, activation functions come in two flavors:\n",
    "       - **internal activation functions**:  these tend to be simple, only slightly non-linear, and are usually done with the purpose of providing the neural network with easy gradients to calculate.  The `ReLU` is the most common example of this type.\n",
    "       - **squashing functions**: these are designed to make data conform to a particular range of values, and are usually used at the end of classification models to turn output into a proper prediction.  There are usually two used:\n",
    "        - **sigmoid:** takes a single number and turns it into a probability. ie `sigmoid(0) = 0.5`.  This is standard for binary classification.\n",
    "        - **softmax:** the big brother of the sigmoid, it allows you to take multiple probabilites and make sure they all add up to 1.  The standard for multi-class classification.\n",
    "   - `model compilation`:  In Scikit-learn models come pre-packaged with specific loss functions (in many ways this defines what the models are).  In Keras, you define what they are manually, and this is to be done at the compilation step.\n",
    "   \n",
    "In addition to this, we also got a chance to take a look at some more sophisticated network layers that do a better job of preserving sequential detail in your data -- a useful feature for time series and NLP problems.  \n",
    "\n",
    "These two layers were:\n",
    "\n",
    " - `SimpleRNN`:  the standard way of capturing sequential dependence in a neural network.  It takes each data point in your sample, passes it through a set of matrix multiplications, and then takes the output of that and uses it as the input to the next character in your sample. So by the time you get to your final character, information from every previous word / timestep leading up to that has been incorporated into it (with the most recent values accounting for more).\n",
    " - `LSTM`:  a slightly more advanced version of the `SimpleRNN`, this layer \"gates\" information at each multiplication to make information that was present early on in the sequence stay relevant for longer.  \n",
    " \n",
    "Each of these can be added into a keras model rather easily.  See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "# a sample neural network with an RNN\n",
    "rnn_mod = keras.models.Sequential([\n",
    "      keras.layers.Embedding(10000, 64, input_length = 300),\n",
    "      # notice we don't flatten -- sequential layers typically take 3 layers in -- 2 layers out\n",
    "      keras.layers.SimpleRNN(32),\n",
    "      keras.layers.Dense(64, activation = 'relu'),\n",
    "      keras.layers.Dense(64, activation = 'relu'),\n",
    "      keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# a sample neural network with an LSTM cell\n",
    "lstm_mod = keras.models.Sequential([\n",
    "      keras.layers.Embedding(10000, 64, input_length = 300),\n",
    "      # notice we don't flatten -- sequential layers typically take 3 layers in -- 2 layers out\n",
    "      keras.layers.LSTM(32),\n",
    "      keras.layers.Dense(64, activation = 'relu'),\n",
    "      keras.layers.Dense(64, activation = 'relu'),\n",
    "      keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also add one additional detail to using these layers: how to stack them on top of each other.  If you refer to our previous notebook, you'll recall that an RNN layer returns output data *for each time step*.  This means that for our 300 word reviews, we get 300 sets of outputs.  If you are going to pass the output from this layer into a dense layer, the first 299 are superfluous:  you only need the last one (since it's the final step in a sequence of calculations that make use of the previous inputs).  \n",
    "\n",
    "However, if you want to connect these layers, then you need to provide all of the data for each sequence.  To accommodate this, you need to pass in one additional argument when creating the layers:  `return_sequences = True`.\n",
    "\n",
    "An example is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras model with stacked LSTM cells\n",
    "lstm_mod_stacked = keras.models.Sequential([\n",
    "    keras.layers.Embedding(10000, 64, input_length = 300),\n",
    "    # notice the argument that we're adding in here -- important!\n",
    "    keras.layers.LSTM(32, return_sequences = True),\n",
    "    # no need to do that here since we're passing this into a dense layer\n",
    "    keras.layers.LSTM(32),\n",
    "    keras.layers.Dense(64, activation = 'relu'),\n",
    "    keras.layers.Dense(64, activation = 'relu'),\n",
    "    keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Improvements\n",
    "\n",
    "We'll now discuss three different ways to improve upon our configurations from previous classes:\n",
    " - optimizers (algorithms that adjust your model's weights during training)\n",
    " - using batch sizes \n",
    " - callbacks\n",
    " \n",
    "#### Optimizers\n",
    "After every forward pass in your neural network, your model uses the gradient (error) to update the model weights.  It turns out there are a few different ways to update your model's weights from your gradient.  These different methods are known as optimizers.  This is not the most critical choice you can make, but choosing the right one can help in small ways.  \n",
    "\n",
    "For the time being, there's a near universal choice for the best method to use:  Adam.  (You can read more here:  https://keras.io/api/optimizers/adam/).  It works better because it adaptively changes your learning rate using the momentum from your loss function after each round.\n",
    "\n",
    "To use this in our model, we'll specify it during the compilation step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonat\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# compilation step -- but specifying the adam optimizer\n",
    "lstm_mod.compile(optimizer = keras.optimizers.Adam(learning_rate = 0.0001), \n",
    "                 loss='binary_crossentropy', \n",
    "                 metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks\n",
    "\n",
    "Callbacks are functions you can pass into a model during training to monitor its behavior.  They allow you to take model snapshots, monitor out of sample performance error, and adjust the learning rate throughout training.  \n",
    "\n",
    "Since neural networks can get very complex, it's helpful to be able to adjust their behavior in the middle of training, vs. waiting until the very end to make modifications.  Keras has built in modules for some of the most common uses for them, so let's take a look at them now.\n",
    "\n",
    "We'll add callbacks that allow you to stop the model if you go so many iterations without improvement (early stopping), and also reduce the learning rate if you go so many rounds without an improvement (learning rate annealing).\n",
    "\n",
    "#### Batch Sizes\n",
    "\n",
    "It's better to feed your data to a neural network in small sips vs. big gulps.  What this means is that during the forward pass, you should break your training data into smaller chunks before sending it through.  If your training data is 1000 samples and your batch size is 100, a single epoch will actually consist of 10 different batches of 100.  After each batch, your model will update its weights, making the next batch a little more effective than the last one.  This typically makes training both faster and more accurate.\n",
    "\n",
    "Like layer size, it's best to make this number a multiple of 32.  Batches of 64 are a good starting point for this parameter.\n",
    "\n",
    "Implementations of batch size and callbacks is initiated during the `fit()` method.\n",
    "\n",
    "Please see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now we'll fit -- notice the use of callbacks\n",
    "lstm_mod.fit(X_train, y_train, \n",
    "             epochs = 5, \n",
    "             # specify batch size here\n",
    "             batch_size = 64,\n",
    "             validation_split = 0.2, \n",
    "             # enter your list of callbacks into this argument\n",
    "             callbacks = [\n",
    "                 # the EarlyStopping callback will shut your model down if no improvement is observed after a certain time\n",
    "                 tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                                  # the model will turn itself off after 20 rounds w/ no improvement\n",
    "                                                  patience=20, \n",
    "                                                  # when we are finished, restore weights with the best validation score\n",
    "                                                  restore_best_weights=True),\n",
    "                 # this will adjust the learning rate if we go so many rounds without improvement\n",
    "                 tf.keras.callbacks.ReduceLROnPlateau(monitor = \"val_loss\", patience = 10, verbose = 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn:** Re-run the model from the previous class, but with the following details:\n",
    "\n",
    " - Stack two LSTM layers with 64 columns of weights each before connecting to your dense layers\n",
    " - Use Adam as your optimizer, with a learning rate of 0.001\n",
    " - Fit your model with a batch size of 64, and the following two callbacks:\n",
    "   - Early stopping after 4 rounds of no improvement, restore the best weights\n",
    "   - Adjust the learning rate after 2 rounds of no improvement\n",
    " - Fit the model for 300 rounds of training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
